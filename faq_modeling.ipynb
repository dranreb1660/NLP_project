{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deee7b9d-99a1-42ee-a17a-bddc75aaac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa197b-56f9-41c9-bd96-18cda98c6e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e5f379-0c65-44dc-9903-165df9f3e41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>found_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I change my password?</td>\n",
       "      <td>After you have logged in, you can change your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When will I receive my changed ATM PIN?</td>\n",
       "      <td>You will receive your new ATM PIN by post with...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can I get my newly generated PIN online?</td>\n",
       "      <td>No, for security reasons we send you your ATM ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I register for Autopay?</td>\n",
       "      <td>To register for Autopay: Step 1: Click on the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can Chip Credit cards be used anywhere?</td>\n",
       "      <td>Yes, your HDFC Bank Chip Credit card can be us...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0              How do I change my password?   \n",
       "1   When will I receive my changed ATM PIN?   \n",
       "2  Can I get my newly generated PIN online?   \n",
       "3           How can I register for Autopay?   \n",
       "4   Can Chip Credit cards be used anywhere?   \n",
       "\n",
       "                                              answer  found_duplicate  \n",
       "0  After you have logged in, you can change your ...            False  \n",
       "1  You will receive your new ATM PIN by post with...            False  \n",
       "2  No, for security reasons we send you your ATM ...            False  \n",
       "3  To register for Autopay: Step 1: Click on the ...            False  \n",
       "4  Yes, your HDFC Bank Chip Credit card can be us...            False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_json('./data/HDFC_faq.txt')\n",
    "corpus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d117e8fc-a9fe-45d5-82c4-c6c1bd2d896f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0968d7cf-65ba-43f2-af09-7751d91e37d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I change my password?</td>\n",
       "      <td>After you have logged in, you can change your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When will I receive my changed ATM PIN?</td>\n",
       "      <td>You will receive your new ATM PIN by post with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can I get my newly generated PIN online?</td>\n",
       "      <td>No, for security reasons we send you your ATM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I register for Autopay?</td>\n",
       "      <td>To register for Autopay: Step 1: Click on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can Chip Credit cards be used anywhere?</td>\n",
       "      <td>Yes, your HDFC Bank Chip Credit card can be us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0              How do I change my password?   \n",
       "1   When will I receive my changed ATM PIN?   \n",
       "2  Can I get my newly generated PIN online?   \n",
       "3           How can I register for Autopay?   \n",
       "4   Can Chip Credit cards be used anywhere?   \n",
       "\n",
       "                                              answer  \n",
       "0  After you have logged in, you can change your ...  \n",
       "1  You will receive your new ATM PIN by post with...  \n",
       "2  No, for security reasons we send you your ATM ...  \n",
       "3  To register for Autopay: Step 1: Click on the ...  \n",
       "4  Yes, your HDFC Bank Chip Credit card can be us...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus= corpus[['question', 'answer']]\n",
    "corpus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bba953-f33c-4978-bce5-1957fcfceb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619bd838-992d-45f4-93f0-84b39c06b6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check and drop dupicate questions\n",
    "corpus.drop_duplicates(subset='question', keep='first', inplace=True)\n",
    "corpus.reset_index(drop=True, inplace=True)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6be42e-e552-4c75-a89d-e744f1bea082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, answer]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check and drop Nans\n",
    "corpus[corpus.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224c3f08-3968-47a1-95ed-a9fd37ea0c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            How do I change my password?\n",
       "1                 When will I receive my changed ATM PIN?\n",
       "2                Can I get my newly generated PIN online?\n",
       "3                         How can I register for Autopay?\n",
       "4                 Can Chip Credit cards be used anywhere?\n",
       "                              ...                        \n",
       "2228    How to make payment for Insta Loan / Insta Jum...\n",
       "2229    What is the disbursement time for Insta Loan /...\n",
       "2230             How to check the available credit limit?\n",
       "2231    What is the promo code to be entered in the lo...\n",
       "2232    After loan disbursal, How to check the active ...\n",
       "Name: question, Length: 2233, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f1f4e-e4e7-45ce-9ed3-1fe52f7bbd87",
   "metadata": {},
   "source": [
    "### we can seee there are some slashes which are mostly representing ***or*** , so I will go ahead and re place with 'or'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071f18fa-6b4d-4641-b1d3-cd71b43e0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle corpus to models\n",
    "with open(\"models/corpus.df\", \"wb\") as f:\n",
    "    pkl.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab1afd36-2eaf-4bee-a8c9-85f16f3a15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text1(text):\n",
    "    ''' Make texts lower case, remove text in square bracket, remove punctuation'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\"\"[\\/]\"\"\", ' or ', text)\n",
    "    \n",
    "    # Removes quotation marks.\n",
    "    text = text.replace('\"', \"\")\n",
    "    \n",
    "    # Remove numeric characters.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    \n",
    "    # Remove puncuation.\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    \n",
    "    return text\n",
    "round1 = lambda x: clean_text1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d4ebb-5590-4504-ade1-1e302c01a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['question'].apply(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8e692-e978-43d1-b228-99725c570f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceef4b2-a84d-4cd8-8367-a2a7f3f690c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fed0e93-530e-4e6c-a7f7-b452a811ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in my data\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "STOP_WORDS = [stemmer.stem(stopword) for stopword in stopwords.words(\"english\")]\n",
    "L_STOP_WORDS = [lemmer.lemmatize(stopword) for stopword in stopwords.words(\"english\")]\n",
    "\n",
    "def clean_text1(text):\n",
    "    ''' Make texts lower case, remove text in square bracket, remove punctuation'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\"\"[\\/]\"\"\", ' or ', text)\n",
    "    \n",
    "    # Removes quotation marks.\n",
    "    text = text.replace('\"', \"\")\n",
    "    \n",
    "    # Remove numeric characters.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    \n",
    "    # Remove punctuation.\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a470b-f979-4b6f-a676-3c02d2f42f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(\n",
    "    stop_words=STOP_WORDS,\n",
    "    preprocessor=clean_text1,\n",
    "    tokenizer=tokenizer,\n",
    "    min_df=2,\n",
    "#     max_df=.80\n",
    ")\n",
    "\n",
    "document = corpus['question']\n",
    "doc_term_mtx = cv1.fit_transform(document)\n",
    "vocab = cv1.get_feature_names()\n",
    "doc_term_df = pd.DataFrame(doc_term_mtx.toarray(), columns=vocab)\n",
    "print(doc_term_df.shape)\n",
    "\n",
    "doc_term_df\n",
    "\n",
    "# Do modeling here with the count vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99869ca7-d459-44c9-92de-3995c4e6ef30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40735011-f234-46b4-90de-8f784ce76e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "tfidf1 = TfidfVectorizer(\n",
    "    stop_words=STOP_WORDS,\n",
    "    preprocessor=clean_text1,\n",
    "    tokenizer=tokenizer,\n",
    "    min_df=2,\n",
    "#     max_df=.80\n",
    ")\n",
    "\n",
    "document = corpus['question']\n",
    "doc_term_mtx1 = tfidf1.fit_transform(document)\n",
    "vocab = tfidf1.get_feature_names()\n",
    "doc_term_df1 = pd.DataFrame(doc_term_mtx1.toarray(), columns=vocab)\n",
    "print(doc_term_df1.shape)\n",
    "\n",
    "doc_term_df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d8e4-1e07-4d7e-836c-8d79c6704b62",
   "metadata": {},
   "source": [
    "## Dimensionality reduction and Topic modeling\n",
    "1. #### CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ceb6d1-f1a3-4e31-bd25-a04c3fef12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from numpy.linalg import svd\n",
    "from optht import optht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ef7db-2a2e-4b7c-9084-4ac283201054",
   "metadata": {},
   "source": [
    "* ##### Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdbbab-a9dd-42c3-a70f-ac9767136f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(879)\n",
    "X_svd= tsvd.fit_transform(doc_term_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861a8c0-73f5-40be-914c-8acb574a685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0e1c2-a67d-4109-90ea-7968df15fdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc02c09-530b-4459-b75a-1fc8da081ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_variance_explained_plots(algo):\n",
    "    \n",
    "    var_exp_array = algo.explained_variance_ratio_\n",
    "    n_comps = var_exp_array.shape[0] \n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,figsize=(10,6))\n",
    "    \n",
    "    ax[0].fill_between(range(n_comps), var_exp_array)\n",
    "    ax[0].set_title('Variance Explained by Nth Component')\n",
    "    \n",
    "    ax[1].fill_between(range(n_comps), np.cumsum(var_exp_array))\n",
    "    ax[1].set_title('Cumulative Variance Explained by N Components')\n",
    "    \n",
    "#     plt.savefig('./pca.jpg', dpi=100)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c4f4c-43ef-422d-a001-067be03a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_variance_explained_plots(tsvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766a2ac-b41b-4cbd-9dc0-a46572c00c6e",
   "metadata": {},
   "source": [
    "* #### Above will give us an idea on how many components/ topics we can reduce our dimensions to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7c192-be55-4b5e-82c3-9bf0eda7a41f",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce2c05-e955-4877-b715-fd502391884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(879)\n",
    "X_pca= pca.fit_transform(doc_term_mtx.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431739ce-ed79-4e09-bfb9-739e8127a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26d0c4-a859-4e63-ae59-87775af9903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_variance_explained_plots(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d56a-0c19-4fa2-ad56-a9f4f5c53e49",
   "metadata": {},
   "source": [
    "### Optimal Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09988198-56ce-49b4-9a04-4be47c7615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "from optht import optht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ae486-7d94-47c0-9060-b355dc74c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt = svd(doc_term_mtx.toarray(), full_matrices=False)\n",
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e863c-a9fa-428b-bdb8-48ec95bfea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = optht(doc_term_mtx1.toarray(), sv=s, sigma=None)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8fa95-30ce-49f0-9499-5ffd13d98dd6",
   "metadata": {},
   "source": [
    "This tells that 169 components will be the optimal number of components that will grab the needend infomation without capturiing the noise associated with the data.\n",
    "\n",
    "This is proven by a published Research paper on $ IEEE $ by Gavin and Donoho, 2014. https://ieeexplore.ieee.org/document/6846297 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be111f6-49dd-4009-8b8c-a7d2f9c6a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_n(doc_t_mtx,n):\n",
    "    tsvd = TruncatedSVD(n)\n",
    "    question_topic= tsvd.fit_transform(doc_t_mtx)\n",
    "    print(f'exp variance sum: {tsvd.explained_variance_ratio_.sum()}')\n",
    "    return tsvd, question_topic\n",
    "def nmf_n(doc_t_mtx,n):\n",
    "    nmf = NMF(n)\n",
    "    question_topic= nmf.fit_transform(doc_t_mtx)\n",
    "    return nmf, question_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63b45b-38e5-47f0-b060-d3711949f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(svd_n(doc_term_mtx,169))\n",
    "nmf_n(doc_term_mtx,169)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a235066-3ea4-41f9-ab3f-f3b2b1a0cf1b",
   "metadata": {},
   "source": [
    "* about 76% of our docs explained with the 169 components while truncating the noise in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c08b0b-b8b3-41b7-942a-eac87768f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_topic_df = pd.DataFrame(question_topic).add_prefix('topic_')\n",
    "question_topic_df\n",
    "\n",
    "question_topic_df[['question', 'answer']] = corpus[['question', 'answer']]\n",
    "question_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de907f15-95ea-4351-b9d3-645889ac38cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65369a20-10de-4182-a0c0-0de0948b6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv1.get_feature_names()\n",
    "word_topic_df = pd.DataFrame(tsvd.components_, columns=vocab).T.add_prefix('topic_')\n",
    "word_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5c54b-2d2a-4f40-8935-715066c840ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(question_topic_df.sort_values(by='topic_16', ascending=False).head(10)['question'].values):\n",
    "    print(question)\n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a9269-eee6-4e0c-b798-1f08e19eeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_df.reset_index().sort_values(by='topic_16', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172efaf-4419-48a8-a0c1-79ce9be9dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_questions(question_topic_df, topic, n_questions):\n",
    "    return (question_topic_df\n",
    "            .sort_values(by=topic, ascending=False)\n",
    "            .head(n_questions)['question']\n",
    "            .values)\n",
    "\n",
    "def top_words(word_topic_df, topic, n_words):\n",
    "    return (word_topic_df\n",
    "            .reset_index()\n",
    "            .sort_values(by=topic, ascending=False)\n",
    "            .head(n_words))['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d5164-b784-4a47-afe8-b5fa7329b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_n_questions_and_words(q_topic_df, w_topic_df, n):\n",
    "    for topic in q_topic_df.columns[:-2]:\n",
    "        print(f'\\n{topic}')\n",
    "        print(f'Top {n} questions:')\n",
    "        for q in top_questions(q_topic_df, topic, n):\n",
    "            print(q)\n",
    "        print()\n",
    "        print(f'Top {n} words:')\n",
    "        for word in top_words(w_topic_df, topic, n):\n",
    "            print(word)\n",
    "# show_n_questions_and_words(2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11521a3-614d-40cd-9c6b-59ca42ff501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d6774-7122-4011-bb56-20b3ec7aae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = document.str.lower().str.contains('cancel')\n",
    "document[mask].sample(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab1ad9-5360-464d-9acb-b3f6432c7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in top_words(word_topic_df, 'topic_16', 15):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6f393-2cd8-4bda-9601-c7563caed0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_q = 'How do I cancel my account'\n",
    "embeded_querry = cv1.transform([new_q])\n",
    "embeded_q_topic = tsvd.transform(embeded_querry)\n",
    "\n",
    "res = []\n",
    "for index in range(corpus.shape[0]):\n",
    "    question, embedding = corpus['answer'][index], question_topic[index]\n",
    "    cos_sim = round(cosine_similarity([embedding], embeded_q_topic)[0][0],3)\n",
    "    res.append(cos_sim)\n",
    "#     print(idx)\n",
    "#     print(sim, sent)\n",
    "\n",
    "n = 5\n",
    "idx_array = np.array(res).argsort()[-n:][::-1]\n",
    "answer_idx = idx_array[0]\n",
    "print(f'Top {n} clossest questions:\\n')\n",
    "for i in idx_array:\n",
    "    print(res[i],'---', corpus['question'][i])\n",
    "\n",
    "print(f'\\nQ: {new_q}?')\n",
    "print(f\"Matched: {corpus['question'][answer_idx]}?\\n\")\n",
    "print(f\"Ans: {corpus['answer'][answer_idx]}?\")\n",
    "embeded_querry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552b32f-894a-4715-8eb6-f1cb8f57a1aa",
   "metadata": {},
   "source": [
    "* I will limit to 10 topics now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1f582-9e0d-4e35-bf79-3a579219edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_10 = TruncatedSVD(10)\n",
    "question_topic_10= tsvd_10.fit_transform(doc_term_mtx)\n",
    "tsvd_10.explained_variance_ratio_.sum()\n",
    "question_topic_df_10 = pd.DataFrame(question_topic_10).add_prefix('topic_')\n",
    "\n",
    "question_topic_df_10[['question', 'answer']] = corpus[['question', 'answer']]\n",
    "question_topic_df_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab70867-47a2-4e8e-884d-d6e9d8e8e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv1.get_feature_names()\n",
    "word_topic_df_10 = pd.DataFrame(tsvd_10.components_ , columns=vocab).T.add_prefix('topic_')\n",
    "word_topic_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2a73b-8684-4d59-afe6-d1ce5ed9fbdb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_n_questions_and_words(question_topic_df_10, word_topic_df_10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf9949-3c1c-49df-96e0-8f557c389cf9",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee291cb-d65c-4b24-9c6f-74e268560ee4",
   "metadata": {},
   "source": [
    "### -CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35e33c-616a-4da1-96f7-afb14ac382e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(10)\n",
    "nmf_doc_topic = nmf.fit_transform(doc_term_mtx)\n",
    "\n",
    "nmf_question_topic_df = pd.DataFrame(nmf_doc_topic).add_prefix('topic_')\n",
    "\n",
    "nmf_question_topic_df[['question', 'answer']] = corpus[['question', 'answer']]\n",
    "nmf_question_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752c9a0-c138-4d38-a69c-a9aa15ac443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv1.get_feature_names()\n",
    "nmf_word_topic_df = pd.DataFrame(nmf.components_ , columns=vocab).T.add_prefix('topic_')\n",
    "nmf_word_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1222313-12db-41cd-82ea-8c59af9eafd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_n_questions_and_words(nmf_question_topic_df, nmf_word_topic_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd69e6-a1b2-4a5c-aa28-8ea3d5337ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21887d4c-80b0-4b6f-b81a-44c1f6399ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_vectorize_answer(new_querry, vectorizer, reduction_func):\n",
    "# clean_sent = re.sub(r\"\"\"[\\/]\"\"\", ' or ', new_querry)\n",
    "# clean_sent =  re.sub(r'''[^A-Za-z]+''', ' ', clean_sent)\n",
    "# print(clean_sent)\n",
    "# embeded_querry = vectorizer.transform([clean_sent])\n",
    "# embeded_q_topic = reduction_func.transform(embeded_querry)\n",
    "\n",
    "new_q = 'How do I cancel my account'\n",
    "embeded_querry = cv1.transform([new_q])\n",
    "embeded_q_topic = nmf.transform(embeded_querry)\n",
    "\n",
    "res = []\n",
    "for index in range(corpus.shape[0]):\n",
    "    question, embedding = corpus['answer'][index], nmf_doc_topic[index]\n",
    "    cos_sim = round(cosine_similarity([embedding], embeded_q_topic)[0][0],3)\n",
    "    res.append(cos_sim)\n",
    "#     print(idx)\n",
    "#     print(sim, sent)\n",
    "\n",
    "n = 5\n",
    "idx_array = np.array(res).argsort()[-n:][::-1]\n",
    "answer_idx = idx_array[0]\n",
    "print(f'Top {n} clossest questions:\\n')\n",
    "for i in idx_array:\n",
    "    print(res[i],'---', corpus['question'][i])\n",
    "\n",
    "print(f'\\nQ: {new_q}?')\n",
    "print(f\"Matched: {corpus['question'][answer_idx]}?\\n\")\n",
    "print(f\"Ans: {corpus['answer'][answer_idx]}?\")\n",
    "embeded_querry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285d630-1be5-4d37-89a6-e3c74e0dfc26",
   "metadata": {},
   "source": [
    "## Not satisfied with answer\n",
    "\n",
    "#### Hence I am goig to try TFI-DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c0080-0b1a-47a1-ba98-79511d23fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "tfidf1 = TfidfVectorizer(\n",
    "    stop_words=STOP_WORDS,\n",
    "    preprocessor=clean_text1,\n",
    "    tokenizer=tokenizer,\n",
    "    min_df=2,\n",
    "#     max_df=.80\n",
    ")\n",
    "\n",
    "document = corpus['question']\n",
    "doc_term_mtx_tfidf = tfidf1.fit_transform(document)\n",
    "vocab_tfidf = tfidf1.get_feature_names()\n",
    "doc_term_df1 = pd.DataFrame(doc_term_mtx_tfidf.toarray(), columns=vocab)\n",
    "doc_term_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e63fc-41fd-4707-8a56-676024803d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding optimal thresh\n",
    "u,s,vt = svd(doc_term_mtx.toarray(), full_matrices=False)\n",
    "u.shape, s.shape, vt.shape\n",
    "k = optht(doc_term_mtx_tfidf.toarray(), sv=s, sigma=None)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863f9a8-6dcd-4db5-af60-d70958eaa7dd",
   "metadata": {},
   "source": [
    "### $LSA$($TrucatedSVD)-169$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc001fb-a187-48b4-8be7-5a8079414bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd169, question_topic = svd_n(doc_term_mtx_tfidf,169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7afa0-0aa2-490e-9961-282f7f94ed31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb092a-37ff-41e8-a754-2fcdf5dad57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'how can i cancel my account'\n",
    "def predict(model, ques_topic,new_q): \n",
    "#     model, ques_topic = func\n",
    "    embeded_querry = tfidf1.transform([new_q])\n",
    "    embeded_q_topic = model.transform(embeded_querry)\n",
    "\n",
    "    res = []\n",
    "    for index in range(corpus.shape[0]):\n",
    "        question, embedding = corpus['answer'][index], ques_topic[index]\n",
    "        cos_sim = round(cosine_similarity([embedding], embeded_q_topic)[0][0],3)\n",
    "        res.append(cos_sim)\n",
    "    #     print(idx)\n",
    "    #     print(sim, sent)\n",
    "\n",
    "    n = 5\n",
    "    idx_array = np.array(res).argsort()[-n:][::-1]\n",
    "    answer_idx = idx_array[0]\n",
    "    print(f'Top {n} clossest questions:\\n')\n",
    "    for i in idx_array:\n",
    "        print(res[i],'---', corpus['question'][i])\n",
    "\n",
    "    print(f'\\nQ: {new_q}?')\n",
    "    print(f\"Matched: {corpus['question'][answer_idx]}?\\n\")\n",
    "    print(f\"Ans: {corpus['answer'][answer_idx]}?\")\n",
    "    return embeded_querry\n",
    "\n",
    "predict(svd169, question_topic, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe7091-6b5e-4cba-ad9f-3e879736d877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46095c32-51e1-425c-8ecd-dd4b52a207c4",
   "metadata": {},
   "source": [
    "### $NMF-169$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f919a-2517-492c-8ef3-54470343263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf169, question_topic_nmf169 = nmf_n(doc_term_mtx,169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e5451-4320-412b-a979-d810907e1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(nmf169, question_topic_nmf169, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6ee32-b83a-4934-89aa-f1912282f99e",
   "metadata": {},
   "source": [
    "## I am pretty happy with both results and can be seen the prediction underfits and thinks almost evrything is similar to the question when when we go lower than 169 components and overfits when we go higher because we are adding more noise to the data. So this proves the Gavin and Donoho paper on $IEEE$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a49064-c646-4b55-bb27-96c1b412373c",
   "metadata": {},
   "source": [
    "# * Bonus\n",
    "## Standard scaling my data to see before dimensionatilty reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f2755-5356-41f8-b83a-e8005089816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "scaled_doc_term_mtx_tfidf = sc.fit_transform(doc_term_mtx_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb8f50-a95e-4f25-9a93-a4431d541d47",
   "metadata": {},
   "source": [
    "* ### finding optimal n components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a983af5-80d2-4955-81fe-bd8fca9389ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt = svd(scaled_doc_term_mtx_tfidf, full_matrices=False)\n",
    "u.shape, s.shape, vt.shape\n",
    "k = optht(scaled_doc_term_mtx_tfidf, sv=s, sigma=None)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcf4c1-28b6-49e3-aab5-a5b7f31d0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'how do i cancel my insurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e85da-25c6-4909-a01c-e3ac90e02c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd8, question_topic8 = svd_n(scaled_doc_term_mtx_tfidf,169)\n",
    "predict(svd8, question_topic8, q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1002350-2eed-4a2a-98d3-1ede70558e35",
   "metadata": {},
   "source": [
    "# Let's put everything together - using functions and finaly a Chatbotclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26eb4d6d-bf24-40b6-85f0-21690ed38a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "STOP_WORDS = [stemmer.stem(stopword) for stopword in stopwords.words(\"english\")]\n",
    "L_STOP_WORDS = [lemmer.lemmatize(stopword) for stopword in stopwords.words(\"english\")]\n",
    "\n",
    "def clean_text1(text):\n",
    "    ''' Make texts lower case, remove text in square bracket, remove punctuation'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\"\"[\\/]\"\"\", ' or ', text)\n",
    "    # Removes quotation marks.\n",
    "    text = text.replace('\"', \"\")\n",
    "\n",
    "    # Remove numeric characters.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # Remove punctuation.\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# TFIDF\n",
    "def mods(self):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=STOP_WORDS,\n",
    "        preprocessor=clean_text1,\n",
    "        tokenizer=tokenizer,\n",
    "        min_df=2,\n",
    "    #     max_df=.80\n",
    "    )\n",
    "\n",
    "    model = TruncatedSVD(169)\n",
    "    return tfidf, model\n",
    "\n",
    "document = corpus['question']\n",
    "\n",
    "def train():\n",
    "    tfidf, model  = mods()\n",
    "    doc_term_mtx = tfidf.fit_transform(document)\n",
    "    vocab = tfidf.get_feature_names()\n",
    "    model.fit(doc_term_mtx)\n",
    "    question_topic = model.transform( doc_term_mtx)\n",
    "    \n",
    "    return model, question_topic\n",
    "\n",
    "    \n",
    "def predict(new_q):\n",
    "    model, ques_topic = train()\n",
    "    embeded_querry = tfidf1.transform([new_q])\n",
    "    embeded_q_topic = model.transform(embeded_querry)\n",
    "\n",
    "    res = []\n",
    "    for index in range(corpus.shape[0]):\n",
    "        question, embedding = corpus['answer'][index], ques_topic[index]\n",
    "        cos_sim = round(cosine_similarity([embedding], embeded_q_topic)[0][0],3)\n",
    "        res.append(cos_sim)\n",
    "    #     print(idx)\n",
    "    #     print(sim, sent)\n",
    "\n",
    "    n = 5\n",
    "    idx_array = np.array(res).argsort()[-n:][::-1]\n",
    "    answer_idx = idx_array[0]\n",
    "    print(f'Top {n} clossest questions:\\n')\n",
    "    for i in idx_array:\n",
    "        print(res[i],'---', corpus['question'][i])\n",
    "\n",
    "    print(f'\\nQ: {new_q}?')\n",
    "    print(f\"Matched: {corpus['question'][answer_idx]}?\\n\")\n",
    "    print(f\"Ans: {corpus['answer'][answer_idx]}?\")\n",
    "    ans = f\"Ans: {corpus['answer'][answer_idx]}?\"\n",
    "    return ans\n",
    "    \n",
    "    \n",
    "train()\n",
    "predict('how do i cancel my account?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db408db5-4a17-4277-a8cd-23c88a22deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 clossest questions:\n",
      "\n",
      "0.77 --- How do I cancel my registration?\n",
      "0.751 --- Can the policy be cancelled?\n",
      "0.75 --- Can I cancel a transaction?\n",
      "0.709 --- Can DRFs be rejected? What are the reasons for rejection?\n",
      "0.704 --- What is the cancellation procedure?\n",
      "\n",
      "Q: how do i cancel my account??\n",
      "Matched: How do I cancel my registration??\n",
      "\n",
      "Ans: There are 2 ways to de-register 1) You can request a de-registration of your biller online by logging into NetBanking ---> Bill Payment Tab ---> View/Delete Billers OR 2) You can visit your HDFC Bank branch and submit an application to de-register the selected biller from the ATM-BillPay service.?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ans: There are 2 ways to de-register 1) You can request a de-registration of your biller online by logging into NetBanking ---> Bill Payment Tab ---> View/Delete Billers OR 2) You can visit your HDFC Bank branch and submit an application to de-register the selected biller from the ATM-BillPay service.?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()\n",
    "predict('how do i cancel my account?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d98062c-1098-4d93-8e3d-d26c34d7bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mods():\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=STOP_WORDS,\n",
    "        preprocessor=clean_text1,\n",
    "        tokenizer=tokenizer,\n",
    "        min_df=2,\n",
    "    #     max_df=.80\n",
    "    )\n",
    "\n",
    "    model = TruncatedSVD(169)\n",
    "    return tfidf, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2573cd88-1146-4c2b-ae23-ed63f9d14ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle as pkl\n",
    "\n",
    "class Chatbot():\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.stemmer = stemmer\n",
    "        self.document = document\n",
    "        \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    STOP_WORDS = [stemmer.stem(stopword) for stopword in stopwords.words(\"english\")]\n",
    "\n",
    "    def clean_text1(self, text):\n",
    "        ''' Make texts lower case, remove text in square bracket, remove punctuation'''\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\"\"[\\/]\"\"\", ' or ', text)\n",
    "        # Removes quotation marks.\n",
    "        text = text.replace('\"', \"\")\n",
    "\n",
    "        # Remove numeric characters.\n",
    "        text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "        # Remove punctuation.\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenizer(text):\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    # TFIDF and svd\n",
    "    def mods(self):\n",
    "        tfidf = TfidfVectorizer(\n",
    "            stop_words=STOP_WORDS,\n",
    "            preprocessor=clean_text1,\n",
    "            tokenizer=tokenizer,\n",
    "            min_df=2,\n",
    "        #     max_df=.80\n",
    "        )\n",
    "\n",
    "        model = TruncatedSVD(169)\n",
    "        return tfidf, model\n",
    "\n",
    "    document = corpus['question']\n",
    "\n",
    "    def train(self):\n",
    "        tfidf, model = mods()\n",
    "        doc_term_mtx = tfidf.fit_transform(document)\n",
    "        vocab = tfidf.get_feature_names()\n",
    "#         model = TruncatedSVD(169)\n",
    "        model.fit(doc_term_mtx)\n",
    "        question_topic = model.transform( doc_term_mtx)\n",
    "\n",
    "        return model, question_topic\n",
    "\n",
    "    def predict(self,new_q):\n",
    "        model, ques_topic = train(self)\n",
    "        embeded_querry = tfidf1.transform([new_q])\n",
    "        embeded_q_topic = model.transform(embeded_querry)\n",
    "\n",
    "        res = []\n",
    "        for index in range(corpus.shape[0]):\n",
    "            question, embedding = corpus['answer'][index], ques_topic[index]\n",
    "            cos_sim = round(cosine_similarity([embedding], embeded_q_topic)[0][0],3)\n",
    "            res.append(cos_sim)\n",
    "        #     print(idx)\n",
    "        #     print(sim, sent)\n",
    "\n",
    "        n = 5\n",
    "        idx_array = np.array(res).argsort()[-n:][::-1]\n",
    "        answer_idx = idx_array[0]\n",
    "        print(f'Top {n} clossest questions:\\n')\n",
    "        for i in idx_array:\n",
    "            print(res[i],'---', corpus['question'][i])\n",
    "\n",
    "        print(f'\\nQ: {new_q}?')\n",
    "        print(f\"Matched: {corpus['question'][answer_idx]}?\\n\")\n",
    "        print(f\"Ans: {corpus['answer'][answer_idx]}?\")\n",
    "        ans = f\"Ans: {corpus['answer'][answer_idx]}?\"\n",
    "        return ans\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de677319-efa5-4295-a151-5768c6b8305b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_10860/2392716640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchatbot_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_10860/1323105985.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "chatbot_model = Chatbot(corpus)\n",
    "chatbot_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bfd7606-a6b9-42c2-a0e0-68a40efa9faa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_4621/3462150233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'how do i create a new account?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_4621/3039076403.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, new_q)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0membeded_querry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0membeded_q_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded_querry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "chatbot.predict('how do i create a new account?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "221eaf2a-ba71-41de-93f0-513d1ac2c14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621     When does the source account get debited - at ...\n",
       "1878    What all information is required for User id &...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask =document.str.lower().str.contains('creat')\n",
    "document[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fc0d030-7ac6-4df9-b9d5-c373a5adee37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What all information is required for User id & password creation?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.iloc[1878]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d60b30d-ec24-4a79-bdfa-601932b0fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 clossest questions:\n",
      "\n",
      "0.638 --- What all information is required for User id & password creation?\n",
      "0.609 --- If there is no Login Id/Password, is it secure?\n",
      "0.58 --- Do I need to keep a separate User ID and Password for payment of GVAT and Commercial Tax online?\n",
      "0.558 --- If I have registered two email IDs, on which email ID will I receive the IVR Password ?\n",
      "0.553 --- How to set Prepaid NetBanking Login User ID / Password for the first time?\n",
      "\n",
      "Q: what do i need to create a new user id and password?\n",
      "Matched: What all information is required for User id & password creation??\n",
      "\n",
      "Ans: Following information is required to Create User id and Password: Loan Account NumberLast EMI PaidDate of Birth?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x880 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict('what do i need to create a new user id and password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "434cfbb3-4e98-47bc-ae0b-7d471337c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"models/chatbot.mdl\", \"wb\") as f:\n",
    "    pkl.dump(chatbot_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b759c92-7614-425e-bb0c-2a6a0a1b3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/chatbot.mdl\", \"rb\") as f:\n",
    "    chatbot_model2 = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24800386-f539-4695-85c1-23f46e819dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_10860/2299661992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'how can I cancel my account'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_10860/1323105985.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, new_q)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0membeded_querry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0membeded_q_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded_querry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    " chatbot_model2.predict('how can I cancel my account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d06738a-bb18-4a4b-b178-16a0f2b63995",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/sh8zkxcs0sd0b_z8qd2zwrmm0000gn/T/ipykernel_4621/1353461730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp.pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d446c0-b995-412c-a880-b4deb3f04399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (metis)",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
